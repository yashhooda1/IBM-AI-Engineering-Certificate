{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbba8f320f994b17866135c8e06dd495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5273f90b3cb148f8a952bb45de4ec18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    \"concrete_data_week3/valid\",\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 03:40:20.680261: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2024-08-08 03:40:20.688907: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394325000 Hz\n",
      "2024-08-08 03:40:20.689446: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fa7a4b3470 executing computations on platform Host. Devices:\n",
      "2024-08-08 03:40:20.689492: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2024-08-08 03:40:20.716713: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f46825e6990>,\n",
       " <keras.layers.core.Dense at 0x7f467bf44910>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f470b9f0d50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f47480f4910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f47480f4950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f47053d9d10>,\n",
       " <keras.layers.core.Activation at 0x7f47053d92d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f4705d10e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f47052a5d10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f47051ca2d0>,\n",
       " <keras.layers.core.Activation at 0x7f4705c9bbd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f47051d9650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f47051c5f90>,\n",
       " <keras.layers.core.Activation at 0x7f47051c5ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46fc429790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46fc327c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46fc369f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46fc2df210>,\n",
       " <keras.layers.merge.Add at 0x7f46fc2dff50>,\n",
       " <keras.layers.core.Activation at 0x7f46fc22cc90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46fc1dd5d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46fc1a7c90>,\n",
       " <keras.layers.core.Activation at 0x7f46fc1a7fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46fc0e3c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46fc0b73d0>,\n",
       " <keras.layers.core.Activation at 0x7f46fc0b7f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46dc793690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46dc70cf10>,\n",
       " <keras.layers.merge.Add at 0x7f46dc729210>,\n",
       " <keras.layers.core.Activation at 0x7f46dc6aebd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46dc65e990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46dc627a50>,\n",
       " <keras.layers.core.Activation at 0x7f46dc5ecf50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46dc5a9fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46dc523210>,\n",
       " <keras.layers.core.Activation at 0x7f46dc5232d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46dc440850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46dc425b10>,\n",
       " <keras.layers.merge.Add at 0x7f46dc425f90>,\n",
       " <keras.layers.core.Activation at 0x7f46dc35bd50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46dc2e3e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46dc2d1a90>,\n",
       " <keras.layers.core.Activation at 0x7f46dc2bc1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46dc252e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46dc19ec90>,\n",
       " <keras.layers.core.Activation at 0x7f46dc20be90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46dc142510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46dc05ced0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46dc08aed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46c47b9690>,\n",
       " <keras.layers.merge.Add at 0x7f46c47024d0>,\n",
       " <keras.layers.core.Activation at 0x7f46c4717ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46c46b0090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46c4680a90>,\n",
       " <keras.layers.core.Activation at 0x7f46c4694d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46c4635d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46c4542250>,\n",
       " <keras.layers.core.Activation at 0x7f46c459a890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46c4531b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46c44a9110>,\n",
       " <keras.layers.merge.Add at 0x7f46c44a91d0>,\n",
       " <keras.layers.core.Activation at 0x7f46c43c9750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46c43e9c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46c43ac590>,\n",
       " <keras.layers.core.Activation at 0x7f46c4340f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46c42e2e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46c426d1d0>,\n",
       " <keras.layers.core.Activation at 0x7f46c4245610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46c41dd910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46c4126b10>,\n",
       " <keras.layers.merge.Add at 0x7f46c4156190>,\n",
       " <keras.layers.core.Activation at 0x7f46c40f4410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46a07c6110>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46c406c590>,\n",
       " <keras.layers.core.Activation at 0x7f46c406ce50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46a074fc90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46a06e9450>,\n",
       " <keras.layers.core.Activation at 0x7f46a0731590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46a06b26d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46a0590a50>,\n",
       " <keras.layers.merge.Add at 0x7f46a05c6150>,\n",
       " <keras.layers.core.Activation at 0x7f46a055f0d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46a055f8d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46a04c3ad0>,\n",
       " <keras.layers.core.Activation at 0x7f46a03fa190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46a03fafd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46a03963d0>,\n",
       " <keras.layers.core.Activation at 0x7f46a03da150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46a0372e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46a020c590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46a02dfe50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46a022e8d0>,\n",
       " <keras.layers.merge.Add at 0x7f46a01c1b50>,\n",
       " <keras.layers.core.Activation at 0x7f46a0113c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46a00b5590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46a008ecd0>,\n",
       " <keras.layers.core.Activation at 0x7f46a008e110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683f84290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683f6ae50>,\n",
       " <keras.layers.core.Activation at 0x7f4683f6ab10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683e86c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683e7d310>,\n",
       " <keras.layers.merge.Add at 0x7f4683e7d450>,\n",
       " <keras.layers.core.Activation at 0x7f4683d98950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683d98c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683d13ed0>,\n",
       " <keras.layers.core.Activation at 0x7f4683d13b90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683cb0910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683bf8850>,\n",
       " <keras.layers.core.Activation at 0x7f4683c2a190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683b48590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683b29bd0>,\n",
       " <keras.layers.merge.Add at 0x7f4683b29510>,\n",
       " <keras.layers.core.Activation at 0x7f4683a4aa10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683cb0d10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46839c16d0>,\n",
       " <keras.layers.core.Activation at 0x7f46839c1c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683978d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46838821d0>,\n",
       " <keras.layers.core.Activation at 0x7f46838dc650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683873b10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46837bcb50>,\n",
       " <keras.layers.merge.Add at 0x7f46837ec8d0>,\n",
       " <keras.layers.core.Activation at 0x7f468370a250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46839785d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46836f1a50>,\n",
       " <keras.layers.core.Activation at 0x7f4683683f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f468361dad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683587e50>,\n",
       " <keras.layers.core.Activation at 0x7f4683587d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4683539d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f468349fe50>,\n",
       " <keras.layers.merge.Add at 0x7f468349ff10>,\n",
       " <keras.layers.core.Activation at 0x7f4683437cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46833d9f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683346ed0>,\n",
       " <keras.layers.core.Activation at 0x7f4683539cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46832e6b50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683246450>,\n",
       " <keras.layers.core.Activation at 0x7f4683246fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46831e2210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f468314bed0>,\n",
       " <keras.layers.merge.Add at 0x7f468314bd50>,\n",
       " <keras.layers.core.Activation at 0x7f46830fcd50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f468309a950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4683062f90>,\n",
       " <keras.layers.core.Activation at 0x7f4683062950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4682f936d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4682f76f50>,\n",
       " <keras.layers.core.Activation at 0x7f4682e934d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4682ea9dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4682da6a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4682e0a5d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4682d5b0d0>,\n",
       " <keras.layers.merge.Add at 0x7f4682ca8550>,\n",
       " <keras.layers.core.Activation at 0x7f4682c64ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4682cbcd10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4682c3af90>,\n",
       " <keras.layers.core.Activation at 0x7f4682c3ad50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4682b57050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4682b3fdd0>,\n",
       " <keras.layers.core.Activation at 0x7f4682b3ffd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4682a6e190>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46829d1a50>,\n",
       " <keras.layers.merge.Add at 0x7f46829d1ad0>,\n",
       " <keras.layers.core.Activation at 0x7f468296ec10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4682925210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f46828e8310>,\n",
       " <keras.layers.core.Activation at 0x7f46828e8410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4682821f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4682783650>,\n",
       " <keras.layers.core.Activation at 0x7f46827836d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f46827197d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4682685a10>,\n",
       " <keras.layers.merge.Add at 0x7f4682685e10>,\n",
       " <keras.layers.core.Activation at 0x7f46826311d0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f468296e890>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f47052b8490>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 03:41:15.791736: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n",
      "2024-08-08 03:41:21.148448: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-08-08 03:41:28.408406: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-08-08 03:41:36.008337: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-08-08 03:41:42.410768: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/101 [>.............................] - ETA: 1:47:16 - loss: 0.6973 - acc: 0.6025"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
